{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Demonstrate Ensemble of Vanilla CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IF USING GOOGLE COLAB\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "\n",
    "# Import libraries and ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data as pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the annotations file that contains the label and the image file name\n",
    "labels = pd.read_csv('../data/raw/annotations.csv', header=None, names=['fname','label'])\n",
    "\n",
    "# Shuffle data\n",
    "labels = labels.sample(frac=1).reset_index()\n",
    "\n",
    "# Use a list comprehension to loop over image file names and import one by one and store pixel values\n",
    "x = np.array([image.img_to_array(image.load_img('../data/raw/all/'+fname.lower(), target_size=(128, 128))) for fname in labels['fname']])\n",
    "\n",
    "# Because the names are strings, the neural network only takes in numerical formats so we will one-hot encode the label\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels['label'])\n",
    "y = integer_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define an architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arch_01():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Feed to a densily connected layer for prediction\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.Adam(lr=0.001),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arch_02():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.1))\n",
    "\n",
    "    # Feed to a densily connected layer for prediction\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.Adam(lr=0.001),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arch_03():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.05))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D())\n",
    "    model.add(layers.Dropout(0.05))\n",
    "\n",
    "    # Feed to a densily connected layer for prediction\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizers.Adam(lr=0.001),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment for 10 Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All classification reports will be added here. When we are done we can average the f1 scores\n",
    "reports = []\n",
    "\n",
    "# Apply stratified K-fold ith 10 splits. Stratified means the same distribution of classes than the whole dataset\n",
    "kf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# Just for printing purposes\n",
    "id = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x,y):\n",
    "    print('Kfold iteration {}/10'.format(id))\n",
    "    print('Total images: {} ---- Train images: {} ---- Test images: {}'.format(len(x),len(train_index),len(test_index)))\n",
    "\n",
    "    id += 1 \n",
    "    \n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 rotation_range=10, # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                                 width_shift_range=0.1, # randomly shift images horizontally (fraction of total width)\n",
    "                                 height_shift_range=0.1, \n",
    "                                 shear_range=0.1,\n",
    "                                 zoom_range=0.1)\n",
    "    \n",
    "    datagen.fit(X_train)\n",
    "\n",
    "    # Adjust the learning rate over time. (Like we saw in class!)    \n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "    \n",
    "    # Ensemble Learning\n",
    "    ensemble_predictions = {}\n",
    "    for i in range(2):\n",
    "        print(\"training model:\",i)\n",
    "        model = build_arch_01()\n",
    "        history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 16), epochs = 30, \n",
    "                              validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 16,\n",
    "                              callbacks=[learning_rate_reduction])\n",
    "        \n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_test = y_pred_test.astype(int) # Convert float to int\n",
    "        y_pred_test = y_pred_test.reshape(y_pred_test.shape[0]) # Reshape\n",
    "\n",
    "        ensemble_predictions['build_arch_01_'+str(i)] = y_pred_test\n",
    "\n",
    "    for i in range(2):\n",
    "        print(\"training model:\",i)\n",
    "        model = build_arch_02()\n",
    "        history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 16), epochs = 20, \n",
    "                              validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 16,\n",
    "                              callbacks=[learning_rate_reduction])\n",
    "        \n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_test = y_pred_test.astype(int) # Convert float to int\n",
    "        y_pred_test = y_pred_test.reshape(y_pred_test.shape[0]) # Reshape\n",
    "\n",
    "        ensemble_predictions['build_arch_02_'+str(i)] = y_pred_test\n",
    "\n",
    "    for i in range(3):\n",
    "        print(\"training model:\",i)\n",
    "        model = build_arch_03()\n",
    "        history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 16), epochs = 20, \n",
    "                              validation_data = (X_test,y_test), steps_per_epoch=len(X_train) / 16,\n",
    "                              callbacks=[learning_rate_reduction])\n",
    "        \n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_test = y_pred_test.astype(int) # Convert float to int\n",
    "        y_pred_test = y_pred_test.reshape(y_pred_test.shape[0]) # Reshape\n",
    "\n",
    "        ensemble_predictions['build_arch_03_'+str(i)] = y_pred_test\n",
    "\n",
    "    ensemble_df = pd.DataFrame(ensemble_predictions)\n",
    "\n",
    "    temp_test = ensemble_df.mode(axis=1)[0]\n",
    "\n",
    "    reports.append(classification_report(y_test, temp_test, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We loop over all reports (1 per fold) and then compute the average of all weighted f1 scores\n",
    "final_f1_score = np.mean([rep['weighted avg']['f1-score'] for rep in reports])\n",
    "\n",
    "print('Final F1-Score is: {}%'.format(np.round(final_f1_score*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:smile] *",
   "language": "python",
   "name": "conda-env-smile-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
